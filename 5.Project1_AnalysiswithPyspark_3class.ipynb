{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "252a7838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7192e111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import length\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, StringIndexer\n",
    "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import lit\n",
    "from bs4 import BeautifulSoup\n",
    "from pyspark import keyword_only\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from datetime import datetime\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol \n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType \n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6235287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:26:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Rating').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd5dde6",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ace938a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:===>                                                     (1 + 15) / 16]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.csv(\"shopee_final.csv\",header=True\n",
    "                      ,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a712bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 = df1.withColumn('class',lit('fake'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d0956ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:27:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , processed_text, class\n",
      " Schema: _c0, processed_text, class\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/tony.ng/Documents/DS-ML/final_project/project1/shopee_final.csv\n",
      "+---+--------------------+-------+\n",
      "|_c0|      processed_text|  class|\n",
      "+---+--------------------+-------+\n",
      "|  0|miếng dán hơi_dầy...|neutral|\n",
      "|  1|miếng dán rất_tồi...|neutral|\n",
      "+---+--------------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75e2623f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "934543"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79fcaf03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- processed_text: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f17376be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|   class| count|\n",
      "+--------+------+\n",
      "|positive|636052|\n",
      "| neutral|112092|\n",
      "|negative|186399|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupby('class').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587ebad0",
   "metadata": {},
   "source": [
    "### Precprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e53746f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.withColumn('length', length(df1['processed_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ab983c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:27:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , processed_text, class\n",
      " Schema: _c0, processed_text, class\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/tony.ng/Documents/DS-ML/final_project/project1/shopee_final.csv\n",
      "+---+--------------------+--------+------+\n",
      "|_c0|      processed_text|   class|length|\n",
      "+---+--------------------+--------+------+\n",
      "|  0|miếng dán hơi_dầy...| neutral|    28|\n",
      "|  1|miếng dán rất_tồi...| neutral|    57|\n",
      "|  2|cường_lực trắng m...|negative|    23|\n",
      "|  3|hàng cảm_quan đầu...|negative|    63|\n",
      "|  4|chất_lượng cường_...|negative|   101|\n",
      "+---+--------------------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "594b92e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:27:05 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , processed_text, class\n",
      " Schema: _c0, processed_text, class\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/tony.ng/Documents/DS-ML/final_project/project1/shopee_final.csv\n",
      "+--------+------------------+------------------+\n",
      "|   class|          avg(_c0)|       avg(length)|\n",
      "+--------+------------------+------------------+\n",
      "|positive| 477209.3752444769|44.500778346973405|\n",
      "| neutral| 449924.0700585233| 38.89566112827364|\n",
      "|negative|443789.80439272744| 43.35922194469124|\n",
      "+--------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('class').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68208f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df1.select('processed_text','class','length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7d0c9b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------+\n",
      "|      processed_text|   class|length|\n",
      "+--------------------+--------+------+\n",
      "|miếng dán hơi_dầy...| neutral|    28|\n",
      "|miếng dán rất_tồi...| neutral|    57|\n",
      "|cường_lực trắng m...|negative|    23|\n",
      "|hàng cảm_quan đầu...|negative|    63|\n",
      "|chất_lượng cường_...|negative|   101|\n",
      "|kính chất_lượng k...|negative|    41|\n",
      "|kính cường_lực dở...|negative|   126|\n",
      "|bị_vỡ bắt_làm hoà...|negative|    32|\n",
      "|kính bụi dính kín...|negative|    33|\n",
      "|đóng hàng cường_l...| neutral|    32|\n",
      "|hàng cường_lực xi...|negative|    61|\n",
      "|không_vừa gần_chấ...|negative|    22|\n",
      "|                  bé|negative|     2|\n",
      "|         đo thử chán|negative|    11|\n",
      "|   hàng miếng bảo_vệ|negative|    17|\n",
      "|sản_phẩm miết rất...|negative|    89|\n",
      "|chính_xác sản_phẩ...|negative|   151|\n",
      "|     hàng hãng lưu_ý|negative|    15|\n",
      "|chất_lượng sản_ph...|negative|   122|\n",
      "|cường_lực mô_tả b...|negative|    82|\n",
      "+--------------------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6eefe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_data = data.filter(data['processed_text'].isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61c2e25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33669"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9121783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.filter(data['processed_text'].isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d85e9b",
   "metadata": {},
   "source": [
    "### Feature & Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "380fb963",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BsTextExtractor(Transformer, HasInputCol, HasOutputCol):\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(BsTextExtractor, self).__init__() \n",
    "        kwargs = self._input_kwargs \n",
    "        self.setParams(**kwargs)\n",
    "        \n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None): \n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    def _transform(self, dataset):\n",
    "        def f(s):\n",
    "            cleaned_text = BeautifulSoup(s).text \n",
    "            return cleaned_text\n",
    "        t = StringType()\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = dataset[self.getInputCol()]\n",
    "        return dataset.withColumn(out_col, udf(f, t)(in_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c29777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_extractor = BsTextExtractor(inputCol=\"processed_text\", outputCol=\"cleaned_text\")\n",
    "tokenizer = Tokenizer(inputCol='cleaned_text', outputCol='token_text')\n",
    "stopremove= StopWordsRemover(inputCol='token_text', outputCol='stop_tokens')\n",
    "count_vec = CountVectorizer(inputCol='stop_tokens', outputCol='c_vec')\n",
    "idf = IDF(inputCol='c_vec', outputCol='tf_idf')\n",
    "ham_spam_to_num = StringIndexer(inputCol='class',outputCol='label', handleInvalid='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "680a2bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_up = VectorAssembler(inputCols =['tf_idf','length'],\n",
    "                           outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c521930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_pipe = Pipeline(stages=[ham_spam_to_num,\n",
    "                                  text_extractor,\n",
    "                                  tokenizer,\n",
    "                                  stopremove,\n",
    "                                  count_vec,\n",
    "                                  idf,\n",
    "                                  clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1793128a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:27:34 WARN DAGScheduler: Broadcasting large task binary with size 1157.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:=================================================>      (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:27:59 WARN DAGScheduler: Broadcasting large task binary with size 1158.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cleaner = data_prep_pipe.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eac4664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = cleaner.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd0c6089",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_data.select('label','features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f18affa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:28:00 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  2.0|(83245,[81,235,44...|\n",
      "|  2.0|(83245,[81,139,23...|\n",
      "|  1.0|(83245,[16,40,56,...|\n",
      "|  1.0|(83245,[1,2,49,55...|\n",
      "|  1.0|(83245,[2,25,61,9...|\n",
      "|  1.0|(83245,[2,49,271,...|\n",
      "|  1.0|(83245,[12,24,33,...|\n",
      "|  1.0|(83245,[3,180,553...|\n",
      "|  1.0|(83245,[81,166,27...|\n",
      "|  2.0|(83245,[1,22,45,3...|\n",
      "|  1.0|(83245,[0,1,108,2...|\n",
      "|  1.0|(83245,[49,175,14...|\n",
      "|  1.0|(83245,[45,83244]...|\n",
      "|  1.0|(83245,[88,95,504...|\n",
      "|  1.0|(83245,[1,235,889...|\n",
      "|  1.0|(83245,[0,10,156,...|\n",
      "|  1.0|(83245,[0,1,53,81...|\n",
      "|  1.0|(83245,[1,221,367...|\n",
      "|  1.0|(83245,[0,2,6,25,...|\n",
      "|  1.0|(83245,[3,43,53,8...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 26:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "clean_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91be6233",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|label| count|\n",
      "+-----+------+\n",
      "|  0.0|614122|\n",
      "|  1.0|178959|\n",
      "|  2.0|107793|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_data.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "232c8ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = clean_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d6c877",
   "metadata": {},
   "source": [
    "## Buil Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82661486",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f256bf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:29:29 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:=================================================>      (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:29:56 WARN DAGScheduler: Broadcasting large task binary with size 6.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:29:57 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 36:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(83245,[0,1,2,3,5...|[-427.43693111719...|[1.0,5.7782993051...|       0.0|\n",
      "|  0.0|(83245,[0,1,2,3,5...|[-461.99208736574...|[0.99999999999939...|       0.0|\n",
      "|  0.0|(83245,[0,1,2,3,6...|[-519.25518085957...|[1.0,3.9799456631...|       0.0|\n",
      "|  0.0|(83245,[0,1,2,3,6...|[-798.61314764267...|[1.0,2.7528084083...|       0.0|\n",
      "|  0.0|(83245,[0,1,2,3,6...|[-399.71776716824...|[1.0,1.2549581970...|       0.0|\n",
      "|  0.0|(83245,[0,1,2,3,6...|[-393.95755093125...|[1.0,9.2940590066...|       0.0|\n",
      "|  0.0|(83245,[0,1,2,3,6...|[-150.78717162810...|[1.0,4.7692103554...|       0.0|\n",
      "|  0.0|(83245,[0,1,2,3,6...|[-150.78717162810...|[1.0,4.7692103554...|       0.0|\n",
      "|  0.0|(83245,[0,1,2,3,6...|[-150.78717162810...|[1.0,4.7692103554...|       0.0|\n",
      "|  0.0|(83245,[0,1,2,3,6...|[-150.78717162810...|[1.0,4.7692103554...|       0.0|\n",
      "|  0.0|(83245,[0,1,2,3,6...|[-150.78717162810...|[1.0,4.7692103554...|       0.0|\n",
      "|  0.0|(83245,[0,1,2,3,6...|[-150.78717162810...|[1.0,4.7692103554...|       0.0|\n",
      "|  0.0|(83245,[0,1,2,3,6...|[-150.78717162810...|[1.0,4.7692103554...|       0.0|\n",
      "|  0.0|(83245,[0,1,2,3,6...|[-150.78717162810...|[1.0,4.7692103554...|       0.0|\n",
      "|  0.0|(83245,[0,1,2,3,6...|[-150.78717162810...|[1.0,4.7692103554...|       0.0|\n",
      "|  0.0|(83245,[0,1,2,3,6...|[-150.78717162810...|[1.0,4.7692103554...|       0.0|\n",
      "|  0.0|(83245,[0,1,2,3,6...|[-150.78717162810...|[1.0,4.7692103554...|       0.0|\n",
      "|  0.0|(83245,[0,1,2,3,6...|[-150.78717162810...|[1.0,4.7692103554...|       0.0|\n",
      "|  0.0|(83245,[0,1,2,3,6...|[-118.87810482307...|[1.0,9.0079458786...|       0.0|\n",
      "|  0.0|(83245,[0,1,2,3,6...|[-118.87810482307...|[1.0,9.0079458786...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "0:00:00.000040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes()\n",
    "prediction = nb.fit(train)\n",
    "test_results = prediction.transform(test)\n",
    "start_time = datetime.now()\n",
    "train_time = datetime.now() - start_time  \n",
    "test_results.show()\n",
    "print(train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9dc9e443",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:30:46 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:31:11 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n",
      "+-----+----------+------+\n",
      "|label|prediction| count|\n",
      "+-----+----------+------+\n",
      "|  2.0|       0.0|  8628|\n",
      "|  1.0|       1.0| 32254|\n",
      "|  0.0|       1.0|  8994|\n",
      "|  1.0|       0.0|  6893|\n",
      "|  2.0|       2.0| 15062|\n",
      "|  2.0|       1.0|  8397|\n",
      "|  1.0|       2.0| 14556|\n",
      "|  0.0|       0.0|149839|\n",
      "|  0.0|       2.0| 25417|\n",
      "+-----+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_results.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc491516",
   "metadata": {},
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "abe136e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:34:17 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:34:41 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tony.ng/spark/spark-3.3.0-bin-hadoop3/python/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:35:08 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:35:09 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 45:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[149839.   8994.  25417.]\n",
      " [  6893.  32254.  14556.]\n",
      " [  8628.   8397.  15062.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#important: need to cast to float type, and order by prediction, else it won't work\n",
    "preds_and_labels = test_results.select(['prediction','label']).withColumn('label', F.col('label').cast(FloatType())).orderBy('prediction')\n",
    "\n",
    "#select only prediction and label columns\n",
    "preds_and_labels = preds_and_labels.select(['prediction','label'])\n",
    "\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "print(metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87143897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:53:06 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 78:=============================================>          (13 + 3) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model: 0.750074693228309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 78:====================================================>   (15 + 1) / 16]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "acc_eva_nb = MulticlassClassificationEvaluator()\n",
    "acc_nb = acc_eva_nb.evaluate(test_results)\n",
    "print('Accuracy of model: {}'.format(acc_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d2b16a",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7c030040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:54:54 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 80:=================================================>      (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:55:20 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:55:21 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 82:>                                                       (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:55:25 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/03/28 01:55:25 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 82:=================================================>      (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:55:49 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:55:49 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/03/28 01:55:49 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "23/03/28 01:55:50 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 84:>                                                       (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:55:51 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:55:51 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 86:===>                                                    (1 + 15) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:55:53 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:55:54 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 88:=================================================>      (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:55:55 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 89:>                                                         (0 + 4) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:55:56 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 90:===>                                                    (1 + 15) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:55:58 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:55:59 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 92:===>                                                    (1 + 15) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:56:00 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:56:01 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 94:===>                                                    (1 + 15) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:56:02 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:56:03 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 96:===>                                                    (1 + 15) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:56:04 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 97:=============================>                            (2 + 2) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:56:04 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 98:>                                                       (0 + 16) / 16]\r",
      "\r",
      "[Stage 98:===>                                                    (1 + 15) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:56:06 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:56:06 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 100:=============>                                         (4 + 12) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:56:07 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:56:08 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 102:===>                                                   (1 + 15) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:56:09 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.000140\n",
      "23/03/28 01:56:10 WARN DAGScheduler: Broadcasting large task binary with size 9.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 104:===================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:56:37 WARN DAGScheduler: Broadcasting large task binary with size 9.4 MiB\n",
      "+-----+----------+------+\n",
      "|label|prediction| count|\n",
      "+-----+----------+------+\n",
      "|  2.0|       0.0| 23868|\n",
      "|  1.0|       1.0| 25386|\n",
      "|  0.0|       1.0|  4173|\n",
      "|  1.0|       0.0| 27467|\n",
      "|  2.0|       2.0|  1745|\n",
      "|  2.0|       1.0|  6474|\n",
      "|  1.0|       2.0|   850|\n",
      "|  0.0|       0.0|178960|\n",
      "|  0.0|       2.0|  1117|\n",
      "+-----+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lg = LogisticRegression(maxIter=10, regParam=0.3)\n",
    "pre_lg = lg.fit(train)\n",
    "result_lg = pre_lg.transform(test)\n",
    "start_time = datetime.now()\n",
    "train_time = datetime.now() - start_time  \n",
    "print(train_time)\n",
    "result_lg.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde2535f",
   "metadata": {},
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "77832816",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:56:58 WARN DAGScheduler: Broadcasting large task binary with size 9.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:57:23 WARN DAGScheduler: Broadcasting large task binary with size 9.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 108:================================================>      (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:57:50 WARN DAGScheduler: Broadcasting large task binary with size 9.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:57:50 WARN DAGScheduler: Broadcasting large task binary with size 9.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 112:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[178960.   4173.   1117.]\n",
      " [ 27467.  25386.    850.]\n",
      " [ 23868.   6474.   1745.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#important: need to cast to float type, and order by prediction, else it won't work\n",
    "preds_and_labels_lg = result_lg.select(['prediction','label']).withColumn('label', F.col('label').cast(FloatType())).orderBy('prediction')\n",
    "\n",
    "#select only prediction and label columns\n",
    "preds_and_labels_lg = preds_and_labels_lg.select(['prediction','label'])\n",
    "\n",
    "metrics_lg = MulticlassMetrics(preds_and_labels_lg.rdd.map(tuple))\n",
    "print(metrics_lg.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd4af16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7185057f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 01:58:09 WARN DAGScheduler: Broadcasting large task binary with size 9.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 114:================================================>      (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model: 0.7132097976225282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "acc_eva_lg = MulticlassClassificationEvaluator()\n",
    "acc_lg = acc_eva_lg.evaluate(result_lg)\n",
    "print('Accuracy of model: {}'.format(acc_lg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1230a2",
   "metadata": {},
   "source": [
    "#### Nhận xét:\n",
    "- đối với việc sử dụng Pyspark, thời gian xử lý nhanh hơn\n",
    "- Model Naive bayes cũng cho kết quả tốt hơn so với sử dụng ML thông thường cho bài toán."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
